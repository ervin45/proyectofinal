
.. raw:: latex

   \maketitle

.. contents::


========================================================================
                            Introducción
========================================================================

------------------------------------------------------------------------
Organización del documento
------------------------------------------------------------------------

Para el entendimiento del presente documento, el lector deberá estar
familiarizado con alguna de las herramientas y tecnologías de bases de
datos multidimensionales, conceptos de Data Warehouse como cubos OLAP,
ETL y frameworks para el desarrollo web. El capítulo 1, Introducción,
intenta resumir y organizar estas tecnologías y conceptos.

Para la realización de este proyecto se investigaron diversas
tecnologías, algunas de ellas descartadas en las primeras instancias y
otras forman parte del núcleo del sistema final. El capítulo 2,
Tecnología Investigada, enumera y describe las tecnologías más importantes.

El capítulo 3 se centra en el desarrollo mismo del sistema, las partes
claves que se necesitaron implementar para conseguir un sistema final
que se ajuste a la necesidad del cliente. Los 3 componentes de mayor
importancia del sistema son descriptos en este capítulo, en conjunto
con los desafíos de implementación que el equipo de desarrollo se fue
enfrentando.

La metodología utilizada para el desarrollo fue eXtreme Programing. El
capítulo 4 es la sumarización del Juego de la Planificación de la
metodología. Es un resume de las versiones, iteraciones e historias
que se fueron cumpliendo durante la implementación.

La metodología elegida impone la realización de numerosos Tests, los
mismos son descriptos en el capítulo 5 como parte de la Documentación.

Al final de cada proyecto, el equipo de desarrollo elabora una breve
conclusión, la cual es parte del presente informe. La misma se
encuentra en el capítulo 6.

Por último el capítulo 7 posee Anexos que incluyen las herramientas de
apoyo, fragmentos de código, guías de instalación y licencias
utilizadas en este proyecto.

------------------------------------------------------------------------
Temática
------------------------------------------------------------------------

La Empresa Ricardo C. Bieler se dedica a la comercialización de
vehículos (autos, camionetas y camiones), repuestos y servicios
relacionados con el rubro automotor. Tiene la concesión de venta
exclusiva de Mercedes-Benz, Chrysler, Jeep y Dodge. La empresa cuenta
con 60 empleados en diversas áreas, desde el salón comercial hasta un
importante taller. Posee dos sucursales, una en Reconquista y otra en
Rafaela.

La empresa Ricardo C. Bieler S.A. no cuenta, en la actualidad, con un
sistema de soporte de decisiones en lo que se refiere a la compra y
venta de unidades (camiones, utilitarios y automóviles). En el caso de
los repuestos, los movimientos de entrada y salida se basan en
estimaciones por parte del personal actual con un soporte informático
mínimo.

Dentro de las competencias de un ingeniero en sistema se encuentran el
diseño, análisis e implementación de un sistema de soporte de
decisiones.

El presente proyecto propone utilizar técnicas aprendidas en el
transcurso de la carrera para diseñar un sistema que permita a los
dueños y gerentes de la firma obtener un diagnóstico estratégico de su
negocio.

Los productos existentes que brindan una parte de este tipo de
soluciones son: Agata Report, Jasper, Pentaho Reporting para reportes
y Mondrian, Palo para OLAP (On Line Analytical Processing). Como
soporte de datos para el modelado dimensional, utilizan Data
Warehouses sobre bases de datos como Oracle, MS-SQL Server, Postgre,
MySQL, entre otros.

------------------------------------------------------------------------
Problemática
------------------------------------------------------------------------

En el transcurso de los años en los que funciona la empresa, esta ha
ido acumulando datos acerca de las operaciones que realiza para poder
desenvolverse en su ámbito de negocio. Todos estos datos tan sólo fueron
acumulados en los sistemas internos que utiliza la empresa sin la
posibilidad de realizar un análisis mas completo que permita la toma
de decisiones en un nivel estratégico basado no sólo en intuición del
empresario sino también en datos cuantitativos y objetivos.

La empresa no tenía una manera sencilla y eficaz de analizar sus datos
operacionales. No era posible realizar una comparación de los datos en
diferentes variables en un tiempo razonable que permita la toma de
decisiones. Esto era un problema a la hora de decidir el tipo de
compras en materia de autos, camiones y repuestos para alcanzar
objetivos como ser maximizar las ganancias.

Ricardo C. Bieler S.A. consta de un casa central y dos sucursales en
la provincia. Las decisiones sólo eran posibles desde el punto central
por ser el único que tenía acceso a toda la información de la
empresa. Esto provocaba que el proceso de toma de decisiones estuviera
centralizado en muy pocas personas y los aportes de gerentes de las
sucursales fueran mínimos. Por esta causa se hacia muy necesario tener
la posibilidad de acceder a las funcionalidades del sistema desde
cualquiera de estos puntos de decisión y asi agilizar los mecanismos
decisionales.

La lógica de almacenamiento de información empresarial en los últimos
años ha sido analizada y desarrollada bajo distintas ópticas. Data
Warehouse, propuesta por Bill Inmon y Ralph Kimball, entre otros, a
finales de los '90 demostró ser una de las formas más destacadas de
organizar, almacenar y visualizar la información crítica para ser
llevada a los niveles de decisión estratégica de las organizaciones.
Para la aplicación de esta técnica en la toma de decisiones es
necesario un volumen significativo de datos sensibles (como la
facturación).

En nuestro caso, Ricardo C. Bieler S.A. nos facilitó las bases de
datos de facturación de los últimos siete años, sin esa información
nos hubiese sido imposible concretar este proyecto. Por otro lado el
cliente tiene la imperiosa necesidad de herramientas de diagnóstico y
ayuda en la toma de decisiones para mejorar el funcionamiento de su
empresa, ya que en la actualidad no existe ningún sistema que le
brinde esta funcionalidad en tiempo real.

Debido a las características particulares de la estructura de sistemas
de información de la empresa, es necesario realizar adaptaciones a
medida para la generación de informes personalizados que sean de fácil
generación y consulta. Las herramientas comerciales del mercado no
ofrecen la flexibilidad suficiente para esta tarea.

------------------------------------------------------------------------
Estructura de numeración de repuestos
------------------------------------------------------------------------

Mercedez Benz posee un sistema de identificación de partes muy particular.

Se detalla parte los componentes en el número identificacatorio:

.. image :: numeracion_mercedes_benz.eps

Fue muy importante conocer la nomenclatura de los elementos de
Mercedes Benz, porque la jerarquía de elementos no es intuitiva, al
menos no inicialmente. Se detalla a continuación la jerarquía de los
elementos de tipo A (Elementos Constructivos):

* Grupo Constructivo
* Modelo o Baumuster
* Identificación de pieza
* Numero de modificación

El sistema legacy (Octosis) incorporó campos finales que identifican,
por ejemplo, el proveedor. Logrando en algunos casos hasta 27 números
para caracterizar una pieza en particular. Estos datos fueron usados
para completar las dimensiones de los cubos.

Los grupos constructivos se dividen en 2 partes, la primera de ellas son
números del 00 al 99 que clasifican al elemento de la siguiente manera:

* Piezas de motor: Grupos del 01 al 23, 25 y 54
* Piezas de chasis: Grupos del 24 al 59
* Piezas de cabina/carrocería: Grupos del 60 a 97
* Piezas en general: Grupos 98 y 99

Esta división se expande a un total de 1000 subgrupos, que no viene al
caso mencionarlos en este documento.

------------------------------------------------------------------------
Objetivos y alcances
------------------------------------------------------------------------

El sistema funcionará en las sucursales de Rafaela, Reconquista y casa
central en Santa Fe de Ricardo C. Bieler S.A. y los decisores podrán
consultarlo desde donde deseen a través de Internet.

El sistema permitirá la generación de informes semanales, alarmas y
sugerencias en la toma de decisiones para los gerentes y dueños de la
empresa, acerca de la totalidad de las unidades y repuestos comprados
y vendidos en los últimos siete años.

* Diseñar el Data WareHouse. Se diseñarán las tablas de hechos y
  dimensiones que responderán a los informes.

* Diseñar el ETL. Se desarrollara una herramienta que hará la
  extracción, transformación y carga de información en el Data
  Warehouse.

* Diseñar la arquitectura, esto implica relevar de requisitos no
  funcionales del sistema.

* Gestionar informes de Compra y Venta de Unidades (camiones y autos)

* Gestionar informes de Compra y Venta de Repuestos y accesorios

* Gestionar informes de Stock de Unidades.

* Gestionar informes de Stock de Repuestos.

* Gestionar informes de control de comisiones de taller mecánico.

* Gestionar informes comparativos de ventas entre las sucursales. La
  empresa tiene sucursales en Santa Fe, Rafaela y Reconquista.

* Gestionar informes comparativos de ventas de Mercedes Benz de
  Ricardo C. Bieler con las demás concesionarias del país.

* Gestionar Pronósticos de Ventas. El sistema permitirá realizar
  pronósticos de con distintas técnicas para mejorar la toma de
  decisiones.

* Permitir la gestión de alertas. El sistema debe permitir la
  definición de eventos importantes como vencimientos y encargarse de
  emitir el alerta correspondiente cuando este tiene lugar.

* Gestionar informes de seguimiento de compra en relación a objetivos
  de compra establecidos por los proovedores de unidades,
  principalmente Mercedes Benz y Chrysler.

* Diseñar la red de información que le dará conectividad al sistema de
  información.

* Emisión e impresión de reportes necesarios para la toma de
  decisiones.

* Gestión de usuarios. El sistema permitirá a la empresa definir
  diferentes roles con determinados permisos.



------------------------------------------------------------------------
Software Libre
------------------------------------------------------------------------

Definición de Software Libre
----------------------------

Software libre es la denominación del software que brinda libertad a
los usuarios sobre su producto adquirido y por tanto, una vez
obtenido, puede ser usado, copiado, estudiado, modificado y
redistribuido libremente. Según la Free Software Foundation, el
software libre se refiere a la libertad de los usuarios para ejecutar,
copiar, distribuir, estudiar, cambiar y mejorar el software; de modo
más preciso, se refiere a cuatro libertades de los usuarios del
software: 

:Libertad 0: La libertad de usar el programa, con cualquier propósito.

:Libertad 1: La libertad de estudiar el funcionamiento del programa, y
	  adaptarlo a las necesidades.

:Libertad 2: La libertad de distribuir copias, con lo que se puede
	  ayudar a otros.

:Libertad 3: La libertad de mejorar el programa y hacer públicas las
	  mejoras, de modo que toda la comunidad se beneficie (para la
	  segunda y última libertad mencionadas, el acceso al código
	  fuente es un requisito previo).

Ventajas Genéricas
----------------------

1. Económico

El bajo o nulo coste de los productos libres permiten proporcionar a
las PYMES servicios y ampliar sus infraestructuras sin que se vean
mermados sus intentos de crecimiento por no poder hacer frente al pago
de grandes sumas en licencias.

2. Libertad de uso y redistribución

Las licencias de software libre existentes permiten la instalación del
software tantas veces y en tantas máquinas como el usuario desee.

3. Independencia tecnológica

El acceso al código fuente permite el desarrollo de nuevos productos
sin la necesidad de desarrollar todo el proceso partiendo de cero. El
secreto tecnológico es uno de los grandes frenos y desequilibrios
existentes para el desarrollo en el modelo de propiedad intelectual.

4. Fomento de la libre competencia al basarse en servicios y no licencias

Uno de los modelos de negocio que genera el software libre es la
contratación de servicios de atención al cliente. Este sistema permite
que las compañías que den el servicio compitan en igualdad de
condiciones al no poseer la propiedad del producto del cual dan el
servicio.

Esto, además, produce un cambio que redunda en una mayor atención al
cliente y contratación de empleados, en contraposición a sistemas
mayoritariamente sostenidos por la venta de licencias y desatención
del cliente.

5. Formatos estándar

Los formatos estándar permiten una interoperatividad más alta entre
sistemas, evitando incompatibilidades. Los estándares de facto son
válidos en ocasiones para lograr una alta interoperatividad si se
omite el hecho que estos exigen el pago de royalties a terceros y por
razones de mercado expuestas en el anterior punto no interesa que se
perpetúen mucho tiempo.

6. Sistemas sin puertas traseras y más seguros

El acceso al código fuente permite que tanto hackers como empresas de
seguridad de todo el mundo puedan auditar los programas, por lo que la
existencia de puertas traseras es ilógica ya que se pondría en
evidencia y contraviene el interés de la comunidad que es la que lo
genera.

7. Corrección mas rápida y eficiente de fallos

El funcionamiento e interés conjunto de la comunidad ha demostrado
solucionar mas rápidamente los fallos de seguridad en el software
libre, algo que desgraciadamente en el software propietario es mas
difícil y costoso. Cuando se notifica a las empresas propietarias del
software, éstas niegan inicialmente la existencia de dichos fallos por
cuestiones de imagen y cuando finalmente admiten la existencia de esos
bugs tardan meses hasta proporcionar los parches de seguridad.

8. Métodos simples y unificados de gestión de software

Actualmente la mayoría de distribuciones de Linux incorporan alguno de
los sistemas que unifican el método de instalación de programas,
librerías, etc. por parte de los usuarios. Este sistema de
acceso y gestión del software se hace prácticamente utópico si se
extrapola al mercado propietario.

9. Sistema en expansión

Las ventajas especialmente económicas que aportan las soluciones
libres a muchas empresas y las aportaciones de la comunidad han
permitido un constante crecimiento del software libre, hasta superar
en ocasiones como en el de los servidores web, al mercado propietario.

El software libre ya no es una promesa, es una realidad y se utiliza
en sistemas de producción por algunas de las empresas tecnológicas mas
importantes como IBM, SUN Microsystems, Google, Hewlett-Packard,
etc. Paradojicamente, incluso Microsoft, que posee sus propias
herramientas, emplea GNU Linux en muchos de sus servidores. Podemos
augurar sin lugar a dudas un futuro crecimiento de su empleo y una
consolidación bien merecida.

Ventajas para nuestro proyecto
--------------------------------

En lo que respecta al licenciamiento del sistema resultante, el concenso
por el uso de la Licencia GPL, otorga ventajas a ambas partes, a saber:

:Independencia de cliente/proovedor: Cualquiera de las partes tiene
  la posibilidad de continuar el desarrollo del software, sin la
  necesidad de la otra parte.

:Inclusión de código de otros proyectos GPL: Si llegase a existir
  código GPL que pudiese potenciar el sistema actual, sería
  perfectamente legal incluir parte del mismo en el sistema, sin
  necesidad de pedir autorización ni costos extras. Esto permite una
  expansión rápida y barata del sistema, con aplicaciones existentes o
  aplicaciones que todavía no se han creado.




------------------------------------------------------------------------
Terminología utilizada
------------------------------------------------------------------------

:Base de datos multidimensional: Es una base de datos con tablas que
      representan hechos (ejemplo: ventas, compras) y tablas que
      representan dimensiones sobre esos hechos (ejemplo: tiempo,
      sucursal, cliente).

:Data Warehouse: Colección de datos almacenados orientados a un
      determinado ámbito. Es una copia de las transacciones de
      datos específicamente estructurada para la consulta y el
      análisis.

:OLAP: Es una solución utilizada en el campo de la llamada
       Inteligencia empresarial (o Business Intelligence) cuyo
       objetivo es agilizar la consulta de grandes cantidades de
       datos. Para ello utiliza estructuras multidimensionales (o
       Cubos OLAP) que contienen datos resumidos de grandes Bases de
       Datos o Sistemas Transaccionales (OLTP).

:ETL: Extracción, Transformación y Carga de datos. Referido
      normalmente al proceso de extraer datos de los almacenamientos
      necesarios, realizar un procesamientos o transformación de estos
      y almacenarlos o cargarlos en la estructura del Data Warehouse

:Data Mining: proceso que consiste en la extracción de conocimiento
      procesable, implícito en las bases de datos. Es decir lograr
      sacar conclusiones útiles a partir de un conjunto de datos no
      orientado a éstas.

:XMLA: XML for Analysis. Es un estándar basado en XML utilizado para
       la descripcón de resultados de consultas a bases de datos
       multidimensionales.


-----------------------------------------------------------------------
Data Warehouse
-----------------------------------------------------------------------

Definición
-------------------

Es una colección de datos orientada a un determinado ámbito (empresa,
organización, etc.), integrado, no volátil y variable en el tiempo,
que ayuda a la toma de decisiones en la entidad en la que se
utiliza. Se trata, sobre todo, de un expediente completo de una
organización, más allá de la información transaccional y operacional.

La definición no quedaría completa sin mencionar que la organización
de los datos no es lo unico que implica un Data Warehouse. Además incluye
herramientas de análisis y consulta, y utilidades para la extracción,
transformación y carga de datos de diversos orígenes de datos como
pueden ser sistemas empresariales, webs o planillas de cálculo.

Beneficios
------------------

:Información accesible: los contenidos del Data WareHouse son
	     entendibles y navegables, y el acceso a ellos es
	     caracterizado por el rápido desempeño. Estos
	     requerimientos no tienen fronteras y tampoco límites
	     fijos. Cuando hablamos de entendible significa, que los
	     niveles de la información sean correctos y obvios. Y
	     Navegables significa reconocer el destino en la
	     pantalla y llegar a donde queramos con solo un
	     clic. Rápido desempeño significa bajo tiempo de espera.

:Información consistente: la información de una parte de la
	     organización puede hacerse coincidir con la de otra parte
	     de la organización. Si dos medidas de la organización
	     tienen el mismo nombre, entonces deben significar la
	     misma cosa. Y a la inversa, si dos medidas no significan
	     la misma cosa, entonces son etiquetados
	     diferentes. Información consistente significa,
	     información de alta calidad.

:Información adaptable y elástica: el Data WareHouse esta diseñado
	     para cambios continuos. Cuando se le hacen nuevas
	     preguntas al Data WareHouse, los datos existentes y las
	     tecnologías no cambian ni se corrompen. Si se agregan
	     datos nuevos al Data WareHouse, los datos existentes y
	     las tecnologías tampoco cambian ni se corrompen.

:Toma de decisiones: este es el objetivo fundamental y que justifica
      su existencia. El datawarehose presenta la materia prima para el
      proceso de toma de decisiciones: la información. Por esto el
      Data Warehouse muchas veces se puede considerar el eje de los
      sistemas de soporte de decisiones.

Propiedades
-----------------

:Orientado a temas: Los datos en la base de datos están organizados de
	   manera que todos los elementos de datos relativos al mismo
	   evento u objeto del mundo real están unidos entre sí.

:Variante en el tiempo: Los cambios producidos en los datos a lo largo
	  del tiempo quedan registrados para que los informes que
	  puedan generarse reflejen esas variaciones.

:No volátil: La información no se modifica ni se elimina, una vez
    almacenado un dato, éste se convierte en información de sólo
    lectura, y se mantiene para futuras consultas.

:Integrado: La base de datos contiene datos de sistemas operacionales
	    de la organización, sistemas externos y de cualquier dato
	    que se pueda conseguir en medios digitales. Esta
	    diversidad de datos debe ser consistentes para poder
	    obtener informes confiables y útiles.

Por otro lado una base de datos relacional posee las siguientes diferencias:

:Orientada a las aplicaciones: El diseño esta adaptado a los procesos
	   o funciones que debe gestionar la aplicación.

:Tiempo: Los datos almacenados no deben tener necesariamente un
	 relación temporal o clave temporal.

:Datos volátiles: Los datos además de ser leídos pueden ser
       modificados en cualquier momento que la aplicación lo requiera.


Data Marts
------------

Los Data Marts manejan conceptos similares a los de un Data Warehouse,
pero con la diferencia de que estos éstan orientados a la toma de
decisiones a nivel departamental en lugar de hacerlo para toda la
organización.

La bibliografía posee dos corrientes respecto a este concepto. Los
autores Bill Inmon y Raph Kimball poseen visiones considerablementes
distintas sobre la relación entre Data Warehouse y un Data Mart. Para
Inmon el Data Mart es cargado desde el Data Warehouse mediante
programas de carga, visión TOP-DOWN. Para Kimball el Data Mart es una
porción del Data Warehouse y es cargado directamente desde las bases
de datos operacionales, visión BOTTOM-UP.

Elementos que integran un Data Warehouse
-----------------------------------------

:Metadatos: son datos que que se refieren a los del Data Warehouse. Su
	    función es ayudar a los analistas a localizar los
	    contenidos, guiar a los algoritmos utilizados para la
	    sumarización/agregación y describir el método por el cual
	    el dato fue transformado desde su estado original hasta el
	    que es mantenido en el Data Warehouse.

:ETL: sabemos que los datos que se encuentran en el Data Warehouse
      provienen de muchos orígenes, el encargado de realizar la
      conciliación se llama ETL. Este componente es el encargado de
      realizar la extracción, transformación y carga de los datos
      hacia el Data Warehouse. La extracción consite en obtener los
      datos nuevos de los orígenes. La transformación se encarga de
      cambiar la forma de los datos y adaptarla a la que es necesaria
      en el Data Warehouse, este el punto en todo el proceso que mas
      esfuerzo lleva. La carga es el proceso de escritura de los datos
      transformados en el Data Warehouse, es el único proceso que
      escribe en el almacenamiento del Data Warehouse.

:Monitorización: monitorea los datos en las fuentes e informa al ETL
		 los cambios de interes para el Data Warehouse.

------------------------------------------------------------------------
OLAP
------------------------------------------------------------------------

OLAP es el acrónimo en inglés de procesamiento analítico en línea
(On-Line Analytical Processing). Es una solución utilizada en el campo
de la llamada Inteligencia empresarial (o Business Intelligence) cuyo
objetivo es agilizar la consulta de grandes cantidades de datos. Para
ello utiliza estructuras multidimensionales (o Cubos OLAP) que
contienen datos resumidos de grandes Bases de Datos o Sistemas
Transaccionales (OLTP). Se usa en informes de negocios de ventas,
marketing, informes de dirección, minería de datos y áreas similares.

La razón de usar OLAP para las consultas es la velocidad de
respuesta. Una base de datos relacional almacena entidades en tablas
discretas si han sido normalizadas. Esta estructura es buena en un
sistema OLTP pero para las complejas consultas multitabla es
relativamente lenta. Un modelo mejor para búsquedas, aunque peor desde
el punto de vista operativo, es una base de datos multidimensional.
Éstas pueden verse como bases de datos de una sola tabla, su
peculiaridad es que por cada dimensión tienen un campo (o columna), y
otro campo por cada métrica o hecho, es decir estas tablas almacenan
registros cuyos campos son de la forma:

.. raw:: latex

   $$ (d_1,d_2,d_3,...,f_1,f_2,f_3,...) $$

Donde los campos 

.. raw:: latex

   $ d_i $

hacen referencia a las dimensiones de la tabla, y los campos 

.. raw:: latex

   $ f_i $

a las métricas o hechos que se quiere almacenar, estudiar o analizar.


Cubos OLAP
-----------

Nos referimos a cubos OLAP cuando hablamos de bases de datos
multidimensionales, en las cuales el almacenamiento físico de los
datos se realiza en vectores multidimensionales. Los cubos OLAP se
pueden considerar como una ampliación de las dos dimensiones de una
hoja de cálculo. Por ejemplo, una empresa podría analizar algunos
datos financieros por producto, por período de tiempo, por ciudad, por
tipo de ingresos y de gastos, y mediante la comparación de los datos
reales con un presupuesto. Estos parámetros en función de los cuales
se analizan los datos se conocen como dimensiones. Para acceder a los
datos sólo es necesario indexarlos a partir de los valores de las
dimensiones o ejes.

Almacenar físicamente los datos de esta forma tiene sus pros y sus
contras. Por ejemplo, en estas bases de datos las consultas de
selección son muy rápidas (de hecho, casi en tiempo real). Pero uno de
los problemas más grandes de esta forma de almacenamiento es que una
vez poblada la base de datos ésta no puede recibir cambios en su
estructura. Para ello sería necesario rediseñar el cubo.

En un sistema OLAP puede haber más de tres dimensiones, por lo que a
los cubos OLAP también reciben el nombre de hipercubos. Las
herramientas comerciales OLAP tienen diferentes métodos de creación y
vinculación de estos cubos o hipercubos.

Dimensiones y jerarquías
------------------------

Cada una de las dimensiones de un cubo OLAP puede resumirse mediante
una jerarquía. Por ejemplo si se considera una escala (o dimensión)
temporal "Mayo de 2005" se puede incluir en "Segundo Trimestre de
2005", que a su vez se incluye en "Año 2005". De igual manera, otra
dimensión de un cubo que refleje una situación geográfica, las
ciudades se pueden incluir en regiones, países o regiones mundiales;
los productos podrían clasificarse por categorías, y las partidas de
gastos podrían agruparse en tipos de gastos. En cambio, el analista
podría comenzar en un nivel muy resumido, como por ejemplo el total de
la diferencia entre los resultados reales y lo presupuestado, para
posteriormente descender en el cubo (en sus jerarquías) para poder
observar con un mayor nivel de detalle que le permita descubrir en el
cubo los lugares en los que se ha producido esta diferencia, según los
productos y períodos.

Definición técnica[1]
---------------------

.. [1] Extraído de http://es.wikipedia.org/wiki/Cubo_OLAP

En teoría de bases de datos, un cubo OLAP es una representación
abstracta de la proyección de una relación de un RDBMS (Sistema
administrador de bases de datos relacionales). Dada una relación de
orden N, se considera la posibilidad de una proyección que dispone de
los campos X, Y, Z como clave de la relación y de W como atributo
residual. Categorizando esto como una función se tiene que:

.. raw:: latex

   $$   W : (X,Y,Z) \rightarrow W $$

Los atributos X, Y, Z se corresponden con los ejes del cubo, mientras
que el valor de W devuelto por cada tripleta (X, Y, Z) se corresponde
con el dato o elemento que se rellena en cada celda del cubo.

Debido a que los dispositivos de salida (monitores, impresoras, etc)
sólo cuentan con dos dimensiones, no pueden caracterizar fácilmente
cuatro dimensiones, es más práctico proyectar "rebanadas" o secciones
de los datos del cubo (se dice proyectar en el sentido clásico vector
analítico de reducción dimensional, no en el sentido de SQL, aunque
los dos conceptos son claramente análogos), tal vez la expresión:

.. raw:: latex

   $$    W : (X,Y) \rightarrow W $$

Aunque no se conserve la clave del cubo (al faltar el parámetro Z),
puede tener algún significado semántico, sin embargo, también puede
que una sección de la representación funcional con tres parámetros
para un determinado valor de Z también resulte de interés.

La motivación que hay tras OLAP vuelve a mostrar de nuevo el paradigma
de los informes de tablas cruzadas de los sistema de gestión de base
de datos de los 80. Se puede desear una visualización al estilo de una
hoja de cálculo, donde los valores de X se encuentran en la fila 1,
los valores de Y aparecen en la columna A, y los valores de 

.. raw:: latex

  $ W: (X,Y) \rightarrow W $

se encuentran en las celdas individuales a partir de la celda B2
y desde ahí, hacia abajo y hacia la derecha. Si bien se puede utilizar
el Lenguaje de Manipulación de Datos (o DML) de SQL para mostrar las
tuplas (X,Y,W), este formato de salida no es tan deseable como la
alternativa de tablas cruzadas. El primer método requiere que se
realice una búsqueda lineal para cada par (X,Y) dado, para determinar
el correspondiente valor de W, mientras que el segundo permite
realizar una búsqueda más convenientemente permitiendo localizar el
valor W en la intersección de la columna X apropiada con la fila Y
correspondiente.

Se ha desarrollado el lenguaje MDX (MultiDimensional eXpressions o
expresiones multidimensionales) para poder expresar problemas OLAP de
forma fácil. Aunque es posible traducir algunas sus sentencias a SQL
tradicional, con frecuencia se requieren expresiones SQL poco claras
incluso para las sentencias más simples del MDX. Este lenguaje ha sido
acogido por la gran mayoría de los proveedores de OLAP y se ha
convertido en norma de hecho para estos sistemas.


------------------------------------------------------------------------
Porque Web y no Standalone
------------------------------------------------------------------------

Durante el transcurso de la carrera se dictan varias materias de
programación, las cuales varias de ellas se orientan hacia el
desarrollo de una aplicación 'Standalone'. Nos vimos obligados a
revisar la motivación que existe para este tipo de desarrollos.

Uno de los requisitos del cliente era el acceso remoto del sistema. En
el caso de ser standalone se debía diseñar un protocolo de
comunicaciones entre el repositorio central de datos y los clientes
visualizadores y/o modificadores. La ventaja más importante este caso es
la performance.

En contraposición, el desarrollo web poseé el protocolo ya definido
(HTTP) y los visualizadores remotos son navegadores desarrollados por
terceras partes. La performance del sistema y su consecuente
experiencia negativa para el usuario, debía ser resuelta utilizando
cierto procesamiento en el cliente.

Preferimos elegir una tecnología web y el lenguaje JavaScript para el
procesamiento del lado del cliente, de esta manera mejorar la
experiencia del usuario y ganar escalabilidad.


------------------------------------------------------------------------
Porque Python y no Java
------------------------------------------------------------------------

En el mercado de los lenguajes de programación Java se ha impuesto
gracias al concepto de la máquina virtual multiplataforma, una
plataforma que ofrece muchos servicios y a fuerza de marketing. Esto
no quiere decir que Java deba ser utilizado para cualquier desarrollo
como opción por defecto.

En nuestro proyecto hemos elegido Python por ser un lenguaje que nos
hace posible maximizar nuestra velocidad de desarrollo y nos permite
pensar el problema desde una perspectiva de mayor nivel. Lenguajes
como Java o C suelen ser mas rapidos en la ejecución. A diferencia de
Java, Python esta diseñado para ser una verdadera ayuda para el
desarrollador.

Python presenta una serie de ventajas que lo hacen muy atractivo,
tanto para su uso profesional como para el aprendizaje de la
programación. Está en un proceso de continuo desarrollo por una amplia
comunidad de desarrolladores.

Se puede destacar de Python:

:Tiene una librería estándar extensa: usada para una diversidad de
       tareas. Esto viene de la filosofía "baterías incluidas"
       ("batteries included") para módulos de Python.

:Es un lenguaje muy expresivo: es decir, los programas Python son muy
    compactos: un programa Python suele ser bastante más corto que su
    equivalente en lenguajes como Java. Python es considerado
    por muchos un lenguaje de programación de muy alto nivel.

:Es muy legible: la sintaxis de Python es muy elegante y permite la
    escritura de programas cuya lectura resulta más fácil que si
    utilizaremos otros lenguajes de programación. Esto es muy
    importante a la hora de realizar el mantenimiento en el
    futuro. Python ofrece un entorno interactivo que facilita la
    realización de pruebas y ayuda a despejar dudas acerca de ciertas
    características del lenguaje.

:Es un lenguaje de programación multiparadigma: esto significa que más
    que forzar a los programadores a adoptar un estilo particular de
    programación, permite varios estilos: programación orientada a
    objetos, programación estructurada y programación funcional. Posee
    un rico juego de estructuras de datos que se pueden manipular de
    modo sencillo. Python usa tipado de datos dinámico y fuerte.

:Facil de extender: nuevos módulos se pueden escribir fácilmente en C
       o C++. Python puede utilizarse como un lenguaje de extensión
       para módulos y aplicaciones que necesitan de una interfaz
       programable.

Resumiendo, Python es un lenguaje ideal para realizar desarrollos en
un fracción de tiempo respecto a la mayoria de los lenguajes del
mercado reduciendo considerablemente los costos de desarrollo y
lenguajes como Java y C son mas apropiados cuando es muy alto el
requerimiento de performance. En conclusión, pensamos que el tiempo de
desarrollo y la facilidad de mantenimiento es un requisito mucho mas
crítico que la performance para nuestro proyecto, por lo tanto hemos
elegido Python.

-----------------------------------------------------------------------
Framework Utilizado
-----------------------------------------------------------------------

Para el proyecto se eligio el framework web Django. A continuación se
explicará en que consiste el flujo de información del framework.

Django fue diseñado para promover la alta cohesión y el bajo
acoplamiento entre las piezas de una aplicación. Siguiendo esta
filosofía es fácil hacer cambios en un lugar particular de la
aplicación sin afectar otras piezas.

Núcleo del Framework
---------------------------

Consta básicamente de tres piezas -- la lógica de acceso a la base de
datos, la lógica de negocios, y la lógica de presentación --
comprenden un concepto que a veces es llamado el patrón de arquitectura
de software Modelo-Vista-Controlador (MVC). En este patrón, el
"Modelo" hace referencia al acceso a la capa de datos, la "Vista" se
refiere a la parte del sistema que selecciona qué mostrar y cómo
mostrarlo, y el "Controlador" implica la parte del sistema que decide
cual vista usar, dependiendo de la entrada del usuario, accediendo al
modelo si es necesario.  Django sigue este patrón MVC lo
suficiente como para ser llamado un framework MVC. Aquí
se encuentra más o menos como la M, V y C se separan en Django:

:M: la porción de acceso a la base de datos, es manejada por la capa
    de la bse de datos de Django.  
:V: la porción que selecciona cuales datos mostrar y
    cómo mostrarlos, es manejada por la vista y los templates.  
:C: la porción que delega a la vista dependiendo de la entrada del
    usuario, es manejada por el framework mismo siguiendo las URLConf
    y llamando a la función apropiada de Python para la URL obtenida.

Debido a que la "C" es manejada por el mismo framework y la parte mas
interesante se sucede en los modelos, las plantillas y las vistas,
Django es referenciado como un Framework MTV.

En el patrón de diseño MTV:

:M: significa "Model", la capa de acceso a la base de datos. Esta capa
    contiene toda la información sobre los datos: como acceder a
    estos, como validarlos, cual es el comportamiento que tiene, y las
    relaciones entre los datos. Django lo implementa por medio de un
    ORM (Mapeo objeto/relacional) con un lenguaje de consultas
    bastante flexible.

:T: significa "Template", la capa de presentación. Esta capa contiene
    las decisiones relacionadas a la presentación: como algunas cosas
    son mostradas sobre una página web o otro tipo de
    documento. Django Posee un sistema propio de templates que está
    formado por bloques y filtros que se encargan de visualizar
    adecuadamente objetos provenientes de las "View"

:V: significa "View", la capa de la lógica de negocios. Esta capa
    contiene la lógica que accede al modelo y la delega a la plantilla
    apropiada: puedes pensar en esto como un puente entre el modelos y
    las plantillas. Esta logica esta implementada integramente en
    Python por lo que es posible utilizar cualquiera de las
    construcciones y modulos externos propios del lenguaje.

.. image:: mtv.eps

Si se compara con otros frameworks de desarrollo web MVC, quizás se
pueda considerar que las vistas de Django pueden ser el "controlador"
y las plantillas de Django pueden ser la "vista". Esto es una
confusión desafortuna a raíz de las diferentes interpretaciones de
MVC. En la interpretación de Django de MVC, la "vista" describe los
datos que son presentados al usuario; no necesariamente el cómo se
mostrarán, pero si cuales datos son presentados. En contraste otros
frameworks sugieren que el trabajo del controlador incluya la decisión
de cuales datos son presentados al usuario, mientras que la vista sea
estrictamente el como serán presentados y no cuales.

Ninguna de las interpretaciones es más "correcta" que otras. Lo
importante es entender el concepto subyacente.

Configuración
-------------------

Django aisla toda la configuración en un solo archivo. Aquí se
específica la configuración de la base de datos, los directorios de los
templates y demas archivos multimedia (imagenes, javascript, css,
flash, etc) y demas metainformación del sistema.

Mapeo de Vista/URL
---------------------

Una funcionalidad muy práctica de este framework es la flexibilidad
con la que se puede relacionar una URL con una salida
determinada. Para lograr esto, Django provee un archivo en el que dado una
expresión regular de una URL relaciona esta con una función que sea
capaz de devolver un objeto que represente una respuesta HTTP.

------------------------------------------------------------------------
Metodología elegida
------------------------------------------------------------------------

La programación extrema o eXtreme Programming (XP) es un enfoque de la
ingeniería del Software. Consiste en un conjunto de prácticas que a lo
largo de los años han demostrado ser las mejores prácticas de
desarrollo de software, llevadas al extremo, fundamentadas en un
conjunto de valores.

Es la más destacada de los procesos ágiles de desarrollo de
software. Al igual que éstos, la programación extrema se diferencia de
las metodologías tradicionales principalmente en que pone más énfasis
en la adaptabilidad que en la previsibilidad.

Los defensores de XP consideran que los cambios de requisitos sobre la
marcha son un aspecto natural, inevitable e incluso deseable del
desarrollo de proyectos. Creen que ser capaz de adaptarse a los
cambios de requisitos en cualquier punto de la vida del proyecto es
una aproximación mejor y más realista que intentar definir todos los
requisitos al comienzo del proyecto e invertir esfuerzos después en
controlar los cambios en los requisitos. Se puede considerar la
programación extrema como la adopción de las mejores metodologías de
desarrollo de acuerdo a lo que se pretende llevar a cabo con el
proyecto, y aplicarlo de manera dinámica durante el ciclo de vida del
software.


Valores
-------


XP promueve una serie de valores que sientan los pilares de la
metodología. Estos valores son:


:Comunicación: los desarrolladores necesitan intercambiar información
	       e ideas sobre el proyecto, a los directivos, y a los
	       clientes de forma confiable y fácil. La
	       información debe fluir de manera continua y rápida.

:Sencillez: siempre que sea posible hay que elegir soluciones
	    simples. Esto no significa estar equivocado o aplicar
	    enfoques simplistas.

:Retroalimentación: en todos los niveles las personas deberían obtener
		    una retroalimentación muy rápida sobre lo que
		    hacen. Los clientes, los directivos y los
		    desarrolladores tienen que alcanzar una
		    comprensión común de la meta del proyecto, y
		    también acerca del estado actual del proyecto.

:Valor: cada persona implicada en el proyecto debería de tener el
	valor (y el derecho) de expresar su valoración sobre el
	proyecto. Todos deberían de tener el valor de ser abiertos y
	dejar que todos examinasen e incluso modificasen su trabajo.

Prácticas
---------

XP esta compuesta por un conjunto de prácticas que están estrechamente
relacionadas y que aplicadas todas juntas son capaces de dar soporte a
las debilidades de cada práctica. Estas prácticas no son nuevas, sino
que han venido usando a lo largo de la corta historia del desarrollo
de software, la novedad radica en la utilización de estas prácticas de
forma conjunta y llevadas al extremo.

Las prácticas fundamentales de la metodología son: 

:Desarrollo iterativo e incremental: pequeñas mejoras, unas tras
	    otras.	

:Pruebas unitarias continuas: frecuentemente repetidas y
	 automatizadas. Se aconseja escribir el código de la prueba
	 antes de la codificación.

:Programación en parejas: se recomienda que las tareas de desarrollo
	      se lleven a cabo por dos personas en un mismo puesto. Se
	      supone que la mayor calidad del código escrito de esta
	      manera -el código es revisado y discutido mientras se
	      escribe- es más importante que la posible pérdida de
	      productividad inmediata.

:Cliente in-situ: del equipo de programación con el cliente o
	   usuario Se recomienda que un representante del cliente
	   trabaje junto al equipo de desarrollo.

:Realizar entregas frecuentes: para mejorar la retroalimentación con
	  el cliente, ya que el mismo puede usar el sistema y
	  encontrar requisitos no contemplados originalmente. Al mismo
	  tiempo se prueba y se tiene la posibilidad de encontrar
	  errores prematuramente.


:Refactorizacion del código: es decir, reescribir ciertas partes del
		 código para aumentar su legibilidad y mantenibilidad
		 pero sin modificar su comportamiento. Las pruebas han
		 de garantizar que en la refactorización no se ha
		 introducido ningún fallo.

:Propiedad del código compartida: en vez de dividir la responsabilidad
	   en el desarrollo de cada módulo en grupos de trabajo
	   distintos, este método promueve que todo el personal
	   pueda corregir y extender cualquier parte del proyecto. Las
	   frecuentes pruebas de regresión garantizan que los posibles
	   errores serán detectados.


:Simplicidad en el diseño: es la mejor manera de que las cosas
	     funcionen. Cuando todo funcione se podrá añadir
	     funcionalidad si es necesario. La programación extrema
	     apuesta que en más sencillo hacer algo simple y tener un
	     poco de trabajo extra para cambiarlo si se requiere, que
	     realizar algo complicado y quizás nunca utilizarlo.

Etapas del proceso
--------------------

Cabe aclarar que el orden en el que son citadas las etapas no indica
el orden de seguimiento de estas sino una aproximación. En XP se
trabaja pasando en forma constante de una etapa a la otra en la
medida de lo posible. Las etapas son las siguientes:

Planificación
------------------

Se escriben historias, cuya idea principal es describir un caso de uso
en dos o tres líneas con terminología del cliente. Las historias
sirven para la creación test de aceptación y permiten hacer una
estimación de tiempo de desarrollo.  Se crea un plan de lanzamiento
que debe servir para crear un calendario que todos puedan cumplir y en
cuyo desarrollo hayan participado todas las personas involucradas en
el proyecto. Para la creación de iteraciones del proyecto se usará
como base las historias, que deberán ser priorizadas por el cliente y
estimados los tiempos de desarrollo por los participantes. A esto se
lo denomina plan de la iteración.


Diseño
------


Se eligen los diseños más simples que funcionen.

Se elige una metáfora del sistema para que el nombrado de clases,
etcétera, siga una misma línea, facilitando la reutilización y la
comprensión del código.

Se "refactoriza sin piedad". Básicamente, consiste en no tener miedo
de cambiar un diseño o eliminar un código que ya no sirve, o al menos
que ya no es claramente la mejor solución.

Codificación
------------

El cliente está siempre disponible, a ser posible cara a cara. Éste
debería formar parte del equipo de desarrollo, y esté presente en
todas las fases de XP. La idea es usar el tiempo del cliente para
estas tareas en vez de para que cree una detalladísima especificación
de requisitos, y evitar la entrega de un producto peor que le hará
perder tiempo.

El código se ajustará a unos estándares de codificación, asegurando la
consistencia y facilitando la comprensión y refactorización del
código.  Las pruebas unitarias se codifican antes que el código en sí,
haciéndo que la codificación de este último sea más rápida, y que
cuando se afronte la misma se tenga más claro qué objetivos tiene que
cumplir lo que se va a codificar.

La programación del código se realizará en parejas, para aumentar la
calidad del mismo.

El código se integra de manera frecuente, evitando divergencias en el
desarrollo y permitiendo que todo el mundo trabaje con la última
versión. De esta manera, se evitará pasar grandes periodos de tiempo
integrando el código al final del desarrollo, ya que las
incompatibilidades habrán sido detectadas enseguida.

Se usa la propiedad colectiva del código, lo que se traduce en que
cualquier programador puede cambiar cualquier parte del código. El
objetivo es fomentar la contribución de ideas por parte de todo el
equipo de desarrollo

Pruebas
-------

Todo el código debe tener pruebas unitarias, y debe pasarlas antes de
ser lanzado.

Cuando se encuentra un error de codificación o bug, se desarrollan
pruebas para evitar volver a caer en el mismo.

Se realizan pruebas de aceptación frecuentemente, publicando los
resultados de las mismas. Estas pruebas son generadas a partir de las
historias elegidas para la iteración, y son "pruebas de caja negra",
en las que el cliente verifica el correcto funcionamiento de lo que se
está probando.

Cuando se pasa la prueba de aceptación, se considera que la
correspondiente historia se ha completado.

Documentación
-------------

Al utilizar la metodología XP, el grupo de desarrollo no documenta de
la misma manera que en las metodologías tradicionales. La
documentación relevante en esta metodología es: aquella intrínseca del
código como comentarios, estándares de codificación, las historias de
usuarios, los casos de prueba, entre otros.

Por otra parte, la comunicación oral de manera informal es incentivada
por esta metodología y la formalización por escrito de los
procedimientos es mínima en comparación con las metodologías
tradicionales. Realizaremos documentación de mantenimiento del sistema
para futuras mejoras y expansiones.


-----------------------------------------------------------------------
Porque eXtreme Programming
-----------------------------------------------------------------------

En las primeras instancias del proyecto, se debía tomar una decisión
por una metodología de diseño de sistemas. Una elección popular
hubiera sido utilizar una metodología estructurada o semi
estructurada. Intentando definir la mayor cantidad de elementos desde
el momento de la obtención de requerimientos de usuario e intentar que
esos requerimientos se ajusten lo más posible a las necesidades del
cliente.

En las primeras interacciones con el cliente se notó que ese enfoque
terminaría en un fracaso. En gran parte porque el cliente no había
experimentado un Data Warehouse, ni estaba familiarizado con el
concepto, resultando muy dificultoso cristalizar los requerimientos
del mismo en el momento del arranque del diseño.

Por ser un proyecto de final de carrera, el equipo de desarrollo
buscaba el aprendizaje de tecnologías que tengan futuro
prometedor. Logrando así aprovechar el tiempo invertido en desarrollar
habilidades personales que los ayudaran en proyectos futuros.

Por lo tanto, transitar un camino de tecnologías no conocidas para
resolver problemas poco definidos requiere una metodología que
maximice la adaptabilidad del equipo de desarrollo.

XP representó la mejor oferta dentro de las metodologías conocidas.


-----------------------------------------------------------------------
Testing como actitud de avance
-----------------------------------------------------------------------

Una de las mayores adaptaciones que experimentó el equipo de
desarrollo durante el proyecto fue la incorporación de pruebas
(testing) al desarrollo.

Siguiendo la metodología, para cada funcionalidad a desarrollar es
necesario comenzar con las pruebas (de unidad o funcionales) que
determinarán el conjunto válido de entradas y salidas del sistema a
implementar.

En las primeras etapas del proyecto fue duro el proceso de adaptación,
ya que el equipo debió forzarse a la creación paciente de cada una de
las pruebas. Ni bien comenzaron las etapas posteriores a los primeros
prototipos, los test de unidad eran parte de la rutina de
implementación y se valoró el esfuerzo realizado. Muchos de los
arreglos en el núcleo se realizaron la ver las pruebas de unidad
fallar, sin tener que interacctuar con la aplicación web.

El resultado final de esta actividad son 862 tests de unidad.

-----------------------------------------------------------------------
Integración con Subversion (SVN)
-----------------------------------------------------------------------

A continuación se explicará brevemente la forma en la que se han ido
integrando los cambios a lo largo del proyecto con la utilización de la
herramienta SVN.

Ya sabemos que la metodología XP tiene como una de sus prácticas la
integración del código fuente u otro tipo de archivos que sean
modificados por uno o mas desarrolladores. Para este propósito se
utilizó SVN[2].

.. [2] No es exclusivo de XP sino que puede ser aprovechada por
       cualquier metodología, sea estándar o híbrida.

SVN es un herramienta para el control de versiones de
archivos. Permite sacar fotografías del proyecto en cualquier momento
del desarrollo, volver a cualquiera de esos puntos, mezclar los
cambios realizados por mas de un desarrollador sobre un archivo y
resolver conflictos. Su arquitectura es cliente/servidor. En el
servidor se encuentra el repositorio de datos donde se
almacenarán todas las versiones y en cada uno de los clientes habrá
una copia de trabajo que estará sincronizada con la del servidor.

Lo primero que hay que hacer es crear una copia de trabajo en la
máquina cliente mediante una operación de 'check out'. El ciclo de
trabajo consiste en actualizar, realizar cambios y enviar cambios. La
actualización se realiza con la operación 'update' para obtener los
cambios nuevos en el repositorio. Luego se realizan cambios en la
copia local, entre los más importantes que se pueden hacer sobre un
archivo están: modificar su contenido, marcarlo para agregar o
marcarlo para borrar. 

.. image:: svn-normal.eps

Por último todos estos cambios que se hicieron sobre uno o más
archivos deben ser sincronizados mediante la operación 'commit'. Tanto
en el uso de 'update' como de 'commit' puede surgir lo que se conoce
como un conflicto. Estos conflictos se deben a que más de un
desarrollador modificó la misma parte de un archivo sin haber notado
los cambios que realizó otro. Gracias a SVN es bastante sencillo de
resolver y continuar el desarrollo.

.. image:: svn-merge.eps
.. image:: svn-conflict.eps

------------------------------------------------------------------------
Estándares de codificación adoptados
------------------------------------------------------------------------

Los siguientes estándares de codificación fueron adoptados por el equipo
de desarrollo:

* Indentación con 4 espacios.

* Nombres de variables en minúsculas.

* Nombres de funciones en minúsculas y con guiones bajos (ejemplo:
  obtener_peticion(), realizar_informe()).

* Nombres de clases con notación camello o CapWord (ejemplo: NombreClase).

* Separar las funciones y las definiciones de clases con
  dos líneas en blanco.

* Las definiciones de métodos dentro de una misma clase se separan con
  una línea en blanco.

* Despues de ":" (nuevo bloque) va un retorno de carro.

* A ambos lados de un operador aritmético/lógico hay espacios en blanco.




========================================================================
                        Tecnología Investigada
========================================================================


------------------------------------------------------------------------
Java y aplicaciones Tomcat
------------------------------------------------------------------------

Java es un lenguaje de programación de propósito general orientado a
objetos desarrollado por Sun Microsystems a principios de los años
90. Las aplicaciones Java están típicamente compiladas en un bytecode
que es interpretado o compilado a código nativo para su ejecución.

Las aplicaciones libres existentes con funcionalidades de Data
Warehouse, OLAP o similares, están en su mayoría implementadas en
Java. La suite de herramientas Pentaho[3], posee varias herramientas
para realizar ETL, Data Mining, etc. todas implementadas sobre Tomcat[4].

.. [3] http://www.pentaho.com
.. [4] http://tomcat.apache.org

En el comienzo del proyecto, investigamos la suite Pentaho, en
particular el producto Mondrian[5], y nos resultó muy complicada la
instalación y configuración.

Una vez configurado, la performance de la aplicación Java era muy
pobre. Entonces decidimos ver alternativas.

.. [5] ver capítulo 3 para más detalles

------------------------------------------------------------------------
Python
------------------------------------------------------------------------

Python es un lenguaje de programación creado por Guido van Rossum en
el año 1990[6].

.. [6] http://svn.python.org/view/python/trunk/Misc/HISTORY?rev=51814&view=markup

Es comparado habitualmente con TCL, Perl, Scheme, Java y Ruby. En la
actualidad Python se desarrolla como un proyecto de código abierto,
administrado por la Python Software Foundation. La última versión
estable del lenguaje es actualmente la 2.5.1 (18 de abril de 2007)(Se
anunció la llegada de la versión 3.0 para el 2008).

------------------------------------------------------------------------
XPWeb
------------------------------------------------------------------------

Es una herramienta web para administrar proyectos que siguen la
metodología XP. Con esta herramienta es posible realizar la
planificación, gestionar las metaforas, utilizar un calendario,
imprimir reportes con estadisticas y exportar proyectos. Desarrollada
en el lenguaje PHP y de libre uso y distribucion.

Para la planificación permite representar iteraciones, historias de
usuarios y tareas. Es posible introducir tiempos de estimación para la
realización de las tareas e historias y en base a esto realiza
estadisticas de cumplimientos de fechas límite y advierte en pantalla
acerca de vencimientos proximos.

Al ser una herramienta web es de muy facil instalación y publicación,
gracias a ello es muy sencillo acceder a su contenido y edición por
parte de todo el equipo implicado en el desarrollo y planificación del
proyecto.

------------------------------------------------------------------------
Turbo Gears
------------------------------------------------------------------------

Es un framework de desarrollo web para Python. Subdivide el problema
de una aplicación web en capas que interactuan entre si. La filosofía
del proyecto es la reutilización de tecnologías existentes
maduras. Las tecnologías utilizadas son:

:MochiKit: es una biblioteca para simplificar el desarrollo en Javascript.

:Kid: es un sistema de templates amigable tanto para el diseñador
      gráfico como para el programador.

:CherryPy: es una biblioteca para manejar de manera sencilla el
	   entrada/salida de los sistemas web en Python.

:SQLObject: tratamiento de objetos con persistencia de objetos en
	    bases de datos relacionales sin una ocultación total de la
	    base de datos.

El comienzo del proyecto fue realizado en Turbo Gears pero encontramos
algunas debilidades como por ejemplo:

* En caso de error los informes eran escasos y ambigüos.

* La comunidad de desarrolladores del proyecto no respondía de la
  manera esperada.

* La documentación del proyecto estaba desactualizada e incompleta.

------------------------------------------------------------------------
Django
------------------------------------------------------------------------

Django es un framework para desarrollo web en Python basado en el
patron de diseño MVC (Model-View-Controller). A diferencia de Turbo
Gears no se apoya en tecnologías existentes, sino implementa cada una
de las 3 capas logrando una integración superior.

También posee herramientas para un correcto mapeo entre las URLs a
utilizar en la aplicación y los métodos a ser llamados. De esta forma
se pueden realizar cambios de comportamiento sin que afecte la
perspectiva del usuario/cliente web. Esta característica nos brindó la
posibilidad de prototipado rápido y adaptaciones sucesivas.

Una de las premisas del proyecto es el feedback hacia el programador
en el momento del desarrollo de la aplicacion. Nos sorprendió el nivel
de detalle en los informes de errores y las opciones de depuración de
código, esto nos posibilitó acortar los tiempos de iteración en el
desarrollo.

------------------------------------------------------------------------
Turbo Gear vs. Django
------------------------------------------------------------------------

Luego de probar ambas tecnologías encontramos ventajas y desventajas
en ambas:


===========================  ==================  ===================
    Topico                          Django              Turbo Gear
===========================  ==================  ===================
    Mapeo URL                  Flexible            Rigido
    Servidor de Desarrollo     Estable             Inestable
    Información de Errores     Muy Buena           Regular
    Documentación              Amplia              Escasa
    Comunidad                  Activa              Pasiva   
    Mapeo Objeto/Relacional    Limitado            Potente
===========================  ==================  ===================

Finalmente escogimos Django como Framework de desarrollo para el proyecto. 

Los puntos mas importantes fueron la abundancia de su documentación y lo activa
que es su comunidad de usuarios y desarrolladores.

------------------------------------------------------------------------
Mondrian
------------------------------------------------------------------------

Según su propia descripcion:

"Mondrian es un servidor OLAP escrito en Java. Permite analizar
interactivamente conjuntos de datos de gran tamaño guardados en base
de datos SQL sin escribir SQL."

Mondrian es parte de la suite de Inteligencia de Negocios de Pentaho,
y ofrece una interfaz XMLA para consultas multidimensional. Es el lenguaje
de facto en el área, impuesto por SQL Server 2000 de Microsoft.

Nuestra experiencia fue negativa con Mondrian, principalmente por
la incompatibilidad de las herramientas libres disponibles y su
implementación de XMLA. 

------------------------------------------------------------------------
Exportación de XBase a SQL
------------------------------------------------------------------------

Los datos de la empresa estaban almacenados en datos XBase (conocidas
popularmente por sus archivos .dbf), de acceso lento y sin integridad
referencial.

Inicialmente, el volumen de datos a trabajar era de 2.0 Gigabytes,
distribuidos más de 1000 archivos. Esto requería de una opción más
confiable y veloz, por lo que decidimos migrarlos utilizando
dbf2mysql hacia un motor de base de datos relacional como es MySQL.

Luego utilizamos SQL para extraer los datos y crear las bases de datos
multidimensional.

------------------------------------------------------------------------
AJAX
------------------------------------------------------------------------

AJAX, acrónimo de Asynchronous JavaScript And XML (JavaScript
asíncrono y XML), es una técnica de desarrollo web para crear
aplicaciones interactivas. Actualmente es soportada por la mayoria de los
navegadores web.

Ajax no es una tecnología en sí mismo. En realidad, se trata de la
unión de varias tecnologías que se desarrollan de forma autónoma y que
se unen de formas nuevas y sorprendentes. Entre otras tecnologías que
integran AJAX se encuentran JavaScript, XML y CSS.

En las aplicaciones web tradicionales, las acciones del usuario en la
página desencadenan llamadas al servidor. Una vez procesada la
petición del usuario, el servidor devuelve una nueva página HTML al
navegador del usuario. Mediante el uso de AJAX las aplicaciones se
ejecutan en el cliente, es decir, en el navegador de los usuarios y
mantiene comunicación asíncrona con el servidor en segundo plano. De
esta forma es posible realizar cambios sobre la misma página sin
necesidad de recargarla. Esto significa aumentar la interactividad,
velocidad y usabilidad en la misma.

------------------------------------------------------------------------
PostgreSQL
------------------------------------------------------------------------

PostgreSQL es un sistema de gestión de dases de datos
objeto-relacionales (ORDBMS). En la actualidad es considerada la base
de datos mas potente que tiene origen en el software
libre. Desarrollado bajo licencia BSD (Berkeley Software Distribution)
por una amplia comunidad y empresas comerciales.

Como los mas avanzandos motores de base de datos soporta concurrencia,
transacionalidad, encriptación, clustering, triggers, procedimientos
almacenados, funciones, secuencias, relaciones, reglas, tipos de datos
definidos por usuarios, vistas.

Posee un soporte nativo para varios lenguajes de programación como
PHP, Python, C, Perl, Java, etc. También dispone de un buen sistema de
extensiones por medio del cual existen varios proyectos que extienden
la funcionalidad los Postgres para cumplir multiples objetivos.


========================================================================
                             Desarrollo
========================================================================




------------------------------------------------------------------------
                          Servidor de cubos
------------------------------------------------------------------------


Introducción
------------------------------------------------------------------------

En los primeros meses de este proyecto, la arquitectura inicial
elegida poseía a Mondrian como servidor de cubos. Desgraciadamente, en
la implemenación de esta arquitectura tuvimos dificultades en la
instalación, parametrización y utilización del mismo.

A continuación se presenta una lista resumida de problemas encontrados:

* No teníamos experiencia en la administración del servidor de
  aplicaciones Tomcat.

* Existían diferencias substanciales entre las distintas
  instalaciones, sobre todo en el aspecto de seguridad dependiendo de
  la distribución de GNU/Linux elegida.

* La configuración de los cubos y los cubos virtuales eran realizadas
  en un complejísimo archivo XML donde habitualmente teníamos
  inconsistencias.

* No teníamos experiencia en el lenguaje de consulta MDX y a medida
  que lo utilizábamos nos dábamos cuenta que la implementación en
  Modrian es escasa y errónea en algunas características.

* La implementación del protocolo XMLA (XML+SOAP) es incompatible con
  las bibliotecas de SOAP de Python (SOAPy) y luego de hacer nuestro
  propia implementación, las bibliotecas estándares XML no podían
  procesar la salida de Mondrian (que coincide perfectamente con las
  salidas de SQL server 2000 de Microsoft)

Estos inconvenientes nos llevaron a evaluar la posibilidad de realizar
nuestro propio servidor de cubos utilizando un soporte de base de
datos relacional. Evaluamos el subconjunto de funcionalidades
necesarias y posibles optimizaciones basadas en nuestra necesidad, y
desarrollamos Cubículo.

Cubículo
------------------------------------------------------------------------

La piedra fundamental de Cubículo es la descripción de un determinado
informe a obtener. Los parámetros necesiarios son:

:Hecho: es el conjunto de datos individuales y las columnas a
	relacionarlas con las dimensiones.

:Ejes: son dos dimensiones que serán los ejes de la
       tabla del informe.

:Restricciones: son ecuaciones a realizar sobre los niveles de las
		dimensiones, tanto las elegidas como ejes como otras
		que posee el hecho.

:Medidas: son los elementos individuales que podrán ser sumarizados (u
	  otros tipos de agregación).

:Función: es para el postprocesamiento del cubo.


Cubículo se basa en una base de datos relacional, en este proyecto
la elección fue PostgreSQL, y las tablas de hechos se caracterizan por
tener únicamente Claves Foráneas a las claves de las tablas
dimensionales.

------------------------------------------------------------------------
ETL
------------------------------------------------------------------------

Una de las tareas más complejas en la implementación de un Data
Warehouse es la creación del ETL. Nuestro proyecto demandaba un ETL lo
suficientemente flexible para la interacción de con las bases de datos
operacionales, que inicialmente estaban en formato xBase con una
normalización bastante pobre.

Decidimos implementar los ETLs con applicaciones hechas a medida en
python, para facilitar el acceso a los datos migramos las bases de
datos iniciales a MySQL con dbf2mysql.

.. image :: etl.eps

Uno de los mayores desafíos en esta etapa fue la obtención de
identificadores del Data Warehouse de elementos existentes en las
tablas dimensionales. Implementamos una biblioteca de ayuda a la
consulta o creación de los mismos donde su componenete clave era la
función get_id(), descripta a continuación:

::

  def get_id(cursor, tabla, campoid, campos, filtro=None):

        if filtro:
                rtn = {}
                for x in campos:
                        if x in filtro:
                                rtn[x] = campos[x]
                campos = rtn


        sql = "select  %s from  %s where "% (campoid, tabla)
        arr = []
        for k in campos.keys():
                arr.append("%s = %%s"% k)

        sql += ' and '.join(arr)

        sql_select = sql
        cursor.execute (sql, campos.values())

        try:
		rtn = cursor.fetchone ()[0]
        except:		
                sql = "insert into %s (%s) values (%s)"% 
		     (tabla,
		      ', '.join(campos.keys()),
		      ', '.join(['%s' for x in campos.values()])
		     )
		print sql, campos.values()
		try:
			cursor.execute ('commit')
		except:
			pass

                cursor.execute (sql, campos.values())
		cursor.execute (sql_select, campos.values())
                rtn = cursor.fetchone ()[0]
        return rtn

Los procesos de ETL deben extraer los datos de la base de datos
operacional y llevarlos al Data Warehouse. En nuestro proyecto los
datos estaban vinculados con un catálogo de quinientos mil productos y
ejecución del ETL insumía entre seis y dieciocho horas.

La aplicación de ETL se divide en tres partes lógicas bien diferenciadas: 

:La obtención de los datos: Se debe generar tablas levemente
  desnormalizadas a partir de las bases de datos operacionales
  exportadas a MySQL. Estas tablas poseen varios cientos de
  miles de registros que cubre las posibilidades de cada
  dimension. Para lograr mayor performance, se intentaba utilizar
  operaciones dentro del mismo motor de bases de datos relacional

:La transformación de los datos: Estas tablas debían ser descompuestas
  para anexar datos a las tablas de hecho (fact tables) utilizando los
  registros de las tablas de dimensionales o bien creando nuevos
  registros en las mismas

:La carga del dato (measure): Una vez determinado las claves foráneas
  a utilizar se generaba el comando INSERT para la inclusión del dato
  en la tabla de hecho correspondiente.

Si bien el poceso se puede esquematizar como simple y ordenado, en la
práctica, durante este proyecto se encontraron muchas complicaciones
para validar los datos finales. El cliente poseía algunos informes y
mucha intuición para corroborar los datos almacenados en el data
warehouse. Cada prueba llevaba cerca de un día para generar los cubos
completos y era necesario realizar varias para detectar y corregir posibles
errores de procesamiento.

Se decidió en las primeras etapas del proyecto que la creación del ETL
se realizaría en paralelo al desarrollo de la aplicación
OLAP; logrando así maximizar el uso del tiempo de los desarrolladores.


------------------------------------------------------------------------
Herramienta OLAP
------------------------------------------------------------------------

La finalidad de crear una fuente de datos multidimensional, es para
dar al usuario la posibilidad de inspeccionarla y generar informes sin
la necesidad de crear consultas SQL ad-hoc. Para esta tarea se desarrolló 
una aplicación OLAP en Django.

La principal función de esta aplicación es la visualización de
informes. Sobre estos informes se pueden realizar operaciones de:

:drilldown: Es la operación para obtener mayor grado de detalle sobre
  una dimensión. Ejemplo pasar de año a mes o de rubro a subrubro en
  un informe.

:rollup: Es la operación contraria a drilldown, el usuario puede
  obtener una visión panorámica del informe. Ejemplo: el informe
  visualiza las ventas separadas por vendedor y luego del rollup se
  obtine el informe separado por sucursal.

:dice: Es la operación de intercambio de dimensiones. Ejemplo: el
  informe muestra los datos en los siguientes ejes: pieza y tiempo, y
  luego del dice se visualiza sucursal y tiempo.

:pivot: Realiza un intercambios de los ejes de la tabla, el permite al
  usuario una visualización diferente de los mismos datos que posee.

:drill replacing: Realiza un drilldown agregando restricciones. De
  esta forma si el usuario esta visualizando todos los años y desea
  aumentar la granularidad a mes, pero no quiere que se convierta en
  una explosión de información, puede realizar un drill replacing de
  un año en particuilar.

:slice: Agrega restricciones al informe actual, por ejemplo,
  visualizar solo los datos que pertenezcan a una sucursal o a un area
  geográfica.


En el siguiente gráfico se puede ver la descomposición de las operaciones y
las funciones que debe realizar la instancia de Reporte para lograr retornarle
al usuario la información solicitada:

.. image:: ciclo_de_vida.eps

Para el diseño de estos componentes se utilizó en estilo
arquitectónico denominado Pipe and Filters concatenando las entradas y
las salidas de cada función de manera secuencial. Entonces logramos
dividir el problema en funciones específicas:

:get_sql(): Utiliza Cubículo para obtener el SQL que contiene todos
  los datos. Cubiculo es el único que conoce la estructura relacional
  subyacente.

:exec_sql(): El SQL se debe ejecutar, y realizar comprobaciones de
  sanidad previas y posteriores a la ejecución. El resultado de la
  ejecución suele devolver una gran consulta con unas pocas columnas,
  que luego deberán se reorganizadas para enviarlas al usuario.

:fill_dimensions(): Los datos que existen en el data warehouse puede
  tener dimensaiones incompletas o con huecos. La tarea de
  fill_dimensions() es completar los datos con valores coherentes con
  el informe presentado.

:complete_dimension(): En las operaciones de cubos, siempre es
  necesario tener en ambos cubos las mismas dimensiones para operar. La
  función complete_dimension() se encarga de homogeinizar las
  dimensiones de los cubos involucrados.

:exec_function(): El último paso antes de mostrar el informe es
  aplicar la función de visualización a cada una de las measures
  obtenidas. Esto permite una flexibilidad total al momento de
  confeccionar informes.

========================================================================
Juego de la planificación
========================================================================

.. include:: versiones-iteraciones-historias.txt

========================================================================
                            Documentación
========================================================================

------------------------------------------------------------------------
Diagrama de Entidad-Relacion
------------------------------------------------------------------------

.. image :: DER.eps

------------------------------------------------------------------------
Diagrama de clases
------------------------------------------------------------------------

.. image:: diag_de_clases.eps

.. image:: diag_de_clases_cubiculo.eps

------------------------------------------------------------------------
Tests
------------------------------------------------------------------------

.. include:: doc_test.txt

========================================================================
                             Conclusión
========================================================================

El presente trabajo fue el resultado de diez meses de esfuerzo e
interacción con el cliente, siendo nuestro primer trabajo en conjunto,
fue sin dudas uno de los procesos de aprendizaje más intensos que
tuvimos.

Somos concientes de los costos e implicaciones que sufre una empresa
por el recambio tecnológico. Respetamos la decisión del cliente en
mantener la tecnología con mejor relación costo/beneficio para su
negocio. De todas formas, fue una sorpresa y un desafío interactuar con
las bases de datos operativas (XBase) de tamaño considerable.  Este
elemento nos demostró que la práctica de campo es necesaria para la
transferencia tecnológica entre las unidades académicas y el mercado.

Uno de los aspectos más positivos del proceso fue la utilización de
una metodología en crecimiento como es XP. Nos resultó muy natural la
organización y principios que la metodología propone. También
visualizamos las ventajas de la misma: durante el proceso tuvimos
cambios de rumbo significativos, algunos disparados por el cliente y
otros por nosotros mismos, sin embargo, la replanificación se pudo
realizar en cuestion de minutos.

Durante el diseño y desarrollo del sistema teníamos que
terminar nuestro último año de cursada. En una unidad académica tan
demandante como es nuestra Regional, no fue tarea trivial la
coordinación de los tiempos necesarios para avanzar con el presente
proyecto en paralelo a otras actividades académicas y laborales.  Este
sacrificio fue uno de los mayores aspectos negativos del proyecto,
aunque admitimos que fue grande la satisfacción al ver el resultado
final.

========================================================================
                                Anexos
========================================================================

------------------------------------------------------------------------
Herramientas: Subversion
------------------------------------------------------------------------

El principal objetivo de Subversion (conocido popularmente como SVN)
es la administración de código fuente entre distintos
programadores. Pueden estar separados geográficamente (o bien realizar
cambios de forma asincróna) y mantener de manera simple un repositorio
central y con metadatos relacionados al proceso de desarrollo.

Se pueden restituir versiones anteriores del proyecto como también
generar nuevas ramas de desarrollo, por ejemplo para experimentación
de una funcionalidad en concreto.


------------------------------------------------------------------------
Herramientas: Wiki
------------------------------------------------------------------------

Las anotaciones del proyecto las realizamos con una herramientas web
de filosofía Wiki, provista por Google Code[7]. Esto nos permitió
realizar una base de conocimientos en hipertexto de manera sencilla y
rápida.

.. [7] http://code.google.com/p/proyectofinal



------------------------------------------------------------------------
Herramientas: ReStructured Text (RST)
------------------------------------------------------------------------

Este documento fue generado con ReStructured Text. RST es un lenguaje
de marcas para texto plano util para luego ser convertido a otros
formatos estructurados. 

RST permite inclusión de documentos externos, lo cual nos ayudo para
modularizar la creación del documento. Para la impresión de éste
documento se realizó la conversión al formato Tex, por ser este un
formato que renderiza muy bien los caracteres.



------------------------------------------------------------------------
Guías de instalación de componentes
------------------------------------------------------------------------

La instalación de los componentes necesarios para la ejecución del
proyecto se realizará sobre GNU/Linux. Especificamente se utilizará la
distribución Debian GNU/Linux. La mayoria de las instalaciones y
configuraciones se harán con el sistema de paquetes on-line de Debian
mediante la herramienta apt-get.

Lo primero de todo es copiar el directorio raiz del proyecto a
/local/usr, para esta se necesitaran privilegios de super usuario.

Python
------

::

  apt-get install python25


PostgreSQL
----------


::

  apt-get install postgresql-8.2

Luego se realizará la migración de los datos o se comenzará con una
instancia nueva del sistema.

Es necesario crear un usuario que tenga permisos sobre la base de
datos que contendrá los datos. También se debe tener en cuenta que
postgreSQL debe estar configurado para aceptar conexiones mediante
IP's.

Django
------

Para la instalación de Django tenemos dos opciones. La primera es por
medio de apt-get.

::

  apt-get install python-django

La segunda es mediante el repositorio svn de django.

Primero hay hacer un checkout del repositorio:
::

   svn co http://code.djangoproject.com/svn/django/trunk/ django-trunk

Luego hay que instalar django en el sistema:
::

  cd django-trunk/django
  python setup.py install

Apache
------

::

  apt-get install apache2

Ahora hay que configurar el Apache para que sirva documentos de
django. En el archivo /etc/apache2/http.conf se deben agregar las
siguientes líneas:

::

 <Location ``/bieler/''>
    SetHandler python-program
    PythonHandler django.core.handlers.modpython
    SetEnv DJANGO_SETTINGS_MODULE bielerDjango.settings
    PythonDebug On
    PythonPath ``[`/local/usr'] + sys.path''
 </Location> 
 -------------

Hay que reiniciar el servidor para que pueda tomar los cambios:

::

  /etc/init.d/apache2 restart



Una vez que esta todo instalado y configurado hay ir al navegador e
ingresamos la siguiente url:

http://localhost/bieler/

y ya se puede acceder a la aplicación.


------------------------------------------------------------------------
Licencia GPL
------------------------------------------------------------------------

La licencia pública general de GNU o mas conocida por su nombre en
inglés GNU General Public License es una licencia creada por la Free
Software Foundation a mediados de los 80, y está orientada
principalmente a proteger la libre distribución, modificación y uso de
software. Su propósito es declarar que el software cubierto por esta
licencia es software libre y protegerlo de intentos de apropiación que
restrinjan esas libertadesa los usuarios.

.. raw:: latex

  \begin{thebibliography}{90}
 
  \bibitem{lamport94}
    Leslie Lamport,
    \emph{\LaTeX: A Document Preparation System}.
    Addison Wesley, Massachusetts,
    2nd Edition,
    1994.
   
  \bibitem{pilgrim07}
    Mark Pilgrim,
    \emph{Dive into python},
    http://www.diveintopython.org,
    2004

  \bibitem{ramm}
    Mark Ramm, Kevin Dangoor, Gigi Sayfan,
    \emph{Rapid Web Application with TurboGears;
    Using Python to Create Ajax-Powered Sites},
    Prentice Hall,
    1ra Edicion,
    2005

  \bibitem{amk}
    A. M. Kuchling,
    \emph{XML for Analysis Specification version 1.1},
    http://www.amk.ca/talks/2006-02-07,
    2006

  \bibitem{lundh}
    Fredrik Lundh,
    \emph{Elements and Element Trees},
    http://effbot.org/zone/element.htm
    2007

  \bibitem{dbook}
    Adrian Holovaty, Jacob Kaplan-Moss,
    \emph{The definitive guide to Django}
    Apress,
    1ra Edicion,
    2007

  \bibitem{django}
    Adrian Holovaty, Jacob Kaplan-Moss,
    \emph{The web framework for perfectionists with deadlines},
    http://www.djangoproject.com,
    2008

  \bibitem{apache}
    Roberto Zoia,
    \emph{Como correr Django en Apache},
    http://tinyurl.com/5w4gda,
    2006

  \bibitem{modelo}
    Tomás Casquero,
    \emph{Modelo de datos de Django},
    http://tinyurl.com/5nncvz,
    2008

  \bibitem{mddatasets}
    Microsoft Corporation,
    \emph{Especificacion de MDDataSets},
    http://msdn2.microsoft.com/en-us/library/ms186704.aspx,
    2002

  \bibitem{execute}
    Microsoft Corporation,
    \emph{SQL Server TechCenter, Execute Method(XMLA)},
    http://technet.microsoft.com/en-us/library/ms186691.aspx,
    2007

  \bibitem{soapxml}
    Marek Pikulski,
    \emph{XMLA 1.1 'Execute'-requests - SOAP/XML namespace problems}
    http://tinyurl.com/5vz3wn,
    2004

  \bibitem{psqldoc}
    The PostgreSQL Global Development Group,
    \emph{PostgreSQL, Documentation}
    http://www.postgresql.org/docs/8.3/static/index.html,
    2008

  \bibitem{guiaubuntu}
    \emph{Guia Ubuntu, sección PostgreSQL}
    http://www.guia-ubuntu.org/index.php?title=PostgreSQL,
    2007

  \bibitem{psqlvsmysql}
    Mark Kaletka, Irwin Gaines,
    \emph{PostgreSQL or MySQL?}
    http://www-css.fnal.gov/dsg/external/freeware/pgsql-vs-mysql.html,
    2005

  \bibitem{wikidwh}
    Wikipedia, la enciclopedia libre,
    \emph{Data Warehouse},
    http://en.wikipedia.org/wiki/Data\_warehouse,
    2008

  \bibitem{wikiolap}
    Wikipedia, la enciclopedia libre,
    \emph{OLAP},
    http://en.wikipedia.org/wiki/OLAP,
    2008

  \bibitem{gutierrez}
    Damián Gutiérrez Echeverría, Universidad Iberoamericana
    \emph{Data Warehouse}
    http://tinyurl.com/64nxxf,

  \bibitem{catedra}
    Catedra de Sistemas de Gestion II,
    \emph{Apuntes de la catedra},
    http://tinyurl.com/6m58ta
    2007

  \bibitem{russell}
    Roberta Russell, Bernard W. Taylor III,
    \emph{Operation Management},
    Ed. Prentice Hall,
    1998

  \bibitem{hanke}
    John E. Hanke, Arthur G. Reitsgh,
    \emph{Pronósticos en los negocios},
    Prentice Hall,
    1996
    
  \end{thebibliography}

